{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSbZNAxORrSC1F7jYI7q9d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waqarmm/Intelligent_chatbot_NLP/blob/main/NLP_MODULE_PROJECT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgreKeaF_W3k"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, TrainingArguments, Trainer\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import torch\n",
        "\n",
        "sentiment_mapping = {\n",
        "    \"positive\": 1,\n",
        "    \"Curious to dive deeper\": 2,\n",
        "    \"Disguised\": 3,\n",
        "    \"Fearful\": 4,\n",
        "    \"Happy\": 5,\n",
        "    \"Sad\": 6,\n",
        "    \"Surprised\": 7,\n",
        "}\n",
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/nlp/train-2-2.json', 'r') as json_file:\n",
        "    dataset = json.load(json_file)\n",
        "\n",
        "\n",
        "dataset_list = list(dataset.values())\n",
        "\n",
        "\n",
        "model_name = \"t5-base\"\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./english_only_model\",\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "\n",
        "class ChatbotDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer, max_seq_length=128):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "\n",
        "\n",
        "        messages = item['content']\n",
        "        input_text = ' '.join([f\"{msg['message']}\" for msg in messages])\n",
        "        target_text = messages[-1]['message']  # Use the message from agent_2 as the target\n",
        "\n",
        "\n",
        "        input_tokens = self.tokenizer.encode(input_text,\n",
        "                                             add_special_tokens=True,\n",
        "                                             max_length=self.max_seq_length,\n",
        "                                             truncation=True,\n",
        "                                             padding='max_length',\n",
        "                                             return_tensors='pt')\n",
        "\n",
        "\n",
        "        target_tokens = self.tokenizer.encode(target_text,\n",
        "                                              add_special_tokens=False,\n",
        "                                              max_length=self.max_seq_length,\n",
        "                                              truncation=True,\n",
        "                                              padding='max_length',\n",
        "                                              return_tensors='pt')\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_tokens[0],  # Remove the batch dimension\n",
        "            \"decoder_input_ids\": target_tokens[0],  # Remove the batch dimension\n",
        "        }\n",
        "\n",
        "\n",
        "split_ratio = 0.9  # Adjust this ratio as needed\n",
        "split_index = int(len(dataset_list) * split_ratio)\n",
        "train_dataset = dataset_list[:split_index]\n",
        "val_dataset = dataset_list[split_index:]\n",
        "\n",
        "\n",
        "def tokenize_with_language(dataset, tokenizer, src_lang, tgt_lang, max_seq_length=128):\n",
        "    tokenized_data = []\n",
        "    for item in dataset:\n",
        "        messages = item['content']\n",
        "        input_text = ' '.join([f\"{msg['message']}\" for msg in messages])\n",
        "        sentiment = messages[-1]['sentiment']\n",
        "\n",
        "        input_tokens = tokenizer.encode(\n",
        "            input_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_seq_length,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt',\n",
        "            tgt_lang=\"en\",  # Set target language to English\n",
        "        )\n",
        "\n",
        "        target_text = f\"[SENTIMENT: {sentiment}]\"\n",
        "        target_tokens = tokenizer.encode(\n",
        "            target_text,\n",
        "            add_special_tokens=False,\n",
        "            max_length=max_seq_length,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt',\n",
        "            tgt_lang=\"en\",  # Set target language to English\n",
        "        )\n",
        "\n",
        "        tokenized_data.append({\n",
        "            \"input_ids\": input_tokens[0],\n",
        "            \"decoder_input_ids\": target_tokens[0],\n",
        "        })\n",
        "\n",
        "    return tokenized_data\n",
        "\n",
        "tokenized_train_dataset = tokenize_with_language(train_dataset, tokenizer,src_lang=\"en\",tgt_lang=\"en\")\n",
        "tokenized_val_dataset = tokenize_with_language(val_dataset, tokenizer,src_lang=\"en\",tgt_lang=\"en\")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=None,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        ")\n",
        "\n",
        "\n",
        "def compute_loss(model, inputs):\n",
        "    # Forward pass\n",
        "    outputs = model(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"input_ids\"].ne(tokenizer.pad_token_id),\n",
        "        decoder_input_ids=inputs[\"decoder_input_ids\"],\n",
        "        labels=inputs[\"decoder_input_ids\"],\n",
        "    )\n",
        "    # Extract the loss\n",
        "    loss = outputs.loss\n",
        "    return loss\n",
        "\n",
        "trainer.compute_loss = compute_loss\n",
        "\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "trainer.save_model()\n",
        "model.to('cpu')\n",
        "\n",
        "\n",
        "def generate_response(input_text):\n",
        "\n",
        "    sentiment_scores = analyzer.polarity_scores(input_text)\n",
        "\n",
        "\n",
        "    compound_score = sentiment_scores['compound']\n",
        "    if compound_score >= 0.05:\n",
        "        sentiment_label = \"happy\"\n",
        "    elif compound_score <= -0.05:\n",
        "        sentiment_label = \"Curious to dive deeper\"\n",
        "    else:\n",
        "        sentiment_label = \"neutral\"\n",
        "\n",
        "\n",
        "    sentiment_label_mapped = sentiment_mapping.get(sentiment_label, \"Neutral\")\n",
        "\n",
        "\n",
        "    user_input = f\"User: {input_text}\"\n",
        "    sentiment_text = f\"[SENTIMENT: {sentiment_label_mapped}]\"\n",
        "    input_ids = tokenizer.encode(user_input, sentiment_text,\n",
        "                                 return_tensors=\"pt\", max_length=128,\n",
        "                                 truncation=True, padding=True,\n",
        "                                 tgt_lang=\"en\",  # Set target language to English\n",
        "                                )\n",
        "\n",
        "    response_ids = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_length=128,\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        temperature=0.7,\n",
        "        tgt_lang=\"en\",\n",
        "    )\n",
        "\n",
        "    response_text = tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
        "    return response_text\n",
        "\n",
        "# Example of generating a response\n",
        "user_input = \"I'm feeling great today!\"\n",
        "bot_response = generate_response(user_input)\n",
        "\n",
        "# Print the bot's response\n",
        "print(\"Bot Response:\", bot_response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.to('cpu')\n",
        "\n",
        "# Function for generating responses\n",
        "def generate_response(input_text):\n",
        "    # Detect sentiment using VADER sentiment analysis\n",
        "    sentiment_scores = analyzer.polarity_scores(input_text)\n",
        "  # \"positive\": 1,\n",
        "  #   \"Curious to dive deeper\": 2,\n",
        "  #   \"Disguised\": 3,\n",
        "  #   \"Fearful\": 4,\n",
        "  #   \"Happy\": 5,\n",
        "  #   \"Sad\": 6,\n",
        "  #   \"Surprised\": 7,\n",
        "    # Choose an appropriate sentiment label based on VADER scores\n",
        "    compound_score = sentiment_scores['compound']\n",
        "    if compound_score >= 0.05:\n",
        "        sentiment_label = \"positive\"\n",
        "    elif compound_score <= -0.05:\n",
        "        sentiment_label = \"Curious to dive deeper\"\n",
        "    else:\n",
        "        sentiment_label = \"neutral\"\n",
        "\n",
        "    # Map the sentiment label to the corresponding label used in your dataset\n",
        "    sentiment_label_mapped = sentiment_mapping.get(sentiment_label, \"Neutral\")  # Default to \"Neutral\" if not found\n",
        "\n",
        "    # Generate a response based on the detected sentiment\n",
        "    # user_input = f\"User: {input_text}\"\n",
        "    # sentiment_text = f\"[SENTIMENT: {sentiment_label_mapped}]\"\n",
        "    input_ids = tokenizer.encode(user_input,\n",
        "                                 return_tensors=\"pt\", max_length=128,\n",
        "                                 truncation=True, padding=True,\n",
        "\n",
        "\n",
        "                                )\n",
        "\n",
        "    response_ids = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_length=128,\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        temperature=0.7,\n",
        "\n",
        "    )\n",
        "\n",
        "    response_text = tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
        "    return response_text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#By the way, do you like Fish\n",
        "#Did you know that a seahorse is the only fish to have a neck\n",
        "#What about cats, do you like cats? I'm a dog fan myself.\n",
        "#Have a good day.\n",
        " #Did you know Bruce Lee was a cha cha dancer?\"\n",
        "user_input = \"By the way, do you like Fish\"\n",
        "bot_response = generate_response(user_input)\n",
        "\n",
        "\n",
        "print(\"Bot Response:\", bot_response)"
      ],
      "metadata": {
        "id": "JyDC4V7N_oOD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}